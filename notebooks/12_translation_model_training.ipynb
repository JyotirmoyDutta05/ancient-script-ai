{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "9ba9a41b-bd68-4aed-bf4e-6c2c593398d5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting peft\n",
      "  Using cached peft-0.18.0-py3-none-any.whl.metadata (14 kB)\n",
      "Requirement already satisfied: accelerate in /Users/jyotirmoy/Desktop/Image/ancient-script-ai/ancient-ai-env/lib/python3.13/site-packages (1.12.0)\n",
      "Collecting bitsandbytes\n",
      "  Using cached bitsandbytes-0.42.0-py3-none-any.whl.metadata (9.9 kB)\n",
      "Requirement already satisfied: numpy>=1.17 in /Users/jyotirmoy/Desktop/Image/ancient-script-ai/ancient-ai-env/lib/python3.13/site-packages (from peft) (2.2.6)\n",
      "Requirement already satisfied: packaging>=20.0 in /Users/jyotirmoy/Desktop/Image/ancient-script-ai/ancient-ai-env/lib/python3.13/site-packages (from peft) (25.0)\n",
      "Requirement already satisfied: psutil in /Users/jyotirmoy/Desktop/Image/ancient-script-ai/ancient-ai-env/lib/python3.13/site-packages (from peft) (7.1.3)\n",
      "Requirement already satisfied: pyyaml in /Users/jyotirmoy/Desktop/Image/ancient-script-ai/ancient-ai-env/lib/python3.13/site-packages (from peft) (6.0.3)\n",
      "Requirement already satisfied: torch>=1.13.0 in /Users/jyotirmoy/Desktop/Image/ancient-script-ai/ancient-ai-env/lib/python3.13/site-packages (from peft) (2.9.1)\n",
      "Requirement already satisfied: transformers in /Users/jyotirmoy/Desktop/Image/ancient-script-ai/ancient-ai-env/lib/python3.13/site-packages (from peft) (4.57.3)\n",
      "Requirement already satisfied: tqdm in /Users/jyotirmoy/Desktop/Image/ancient-script-ai/ancient-ai-env/lib/python3.13/site-packages (from peft) (4.67.1)\n",
      "Requirement already satisfied: safetensors in /Users/jyotirmoy/Desktop/Image/ancient-script-ai/ancient-ai-env/lib/python3.13/site-packages (from peft) (0.7.0)\n",
      "Requirement already satisfied: huggingface_hub>=0.25.0 in /Users/jyotirmoy/Desktop/Image/ancient-script-ai/ancient-ai-env/lib/python3.13/site-packages (from peft) (0.36.0)\n",
      "Requirement already satisfied: scipy in /Users/jyotirmoy/Desktop/Image/ancient-script-ai/ancient-ai-env/lib/python3.13/site-packages (from bitsandbytes) (1.16.3)\n",
      "Requirement already satisfied: filelock in /Users/jyotirmoy/Desktop/Image/ancient-script-ai/ancient-ai-env/lib/python3.13/site-packages (from huggingface_hub>=0.25.0->peft) (3.20.0)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in /Users/jyotirmoy/Desktop/Image/ancient-script-ai/ancient-ai-env/lib/python3.13/site-packages (from huggingface_hub>=0.25.0->peft) (2025.10.0)\n",
      "Requirement already satisfied: requests in /Users/jyotirmoy/Desktop/Image/ancient-script-ai/ancient-ai-env/lib/python3.13/site-packages (from huggingface_hub>=0.25.0->peft) (2.32.5)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /Users/jyotirmoy/Desktop/Image/ancient-script-ai/ancient-ai-env/lib/python3.13/site-packages (from huggingface_hub>=0.25.0->peft) (4.15.0)\n",
      "Requirement already satisfied: hf-xet<2.0.0,>=1.1.3 in /Users/jyotirmoy/Desktop/Image/ancient-script-ai/ancient-ai-env/lib/python3.13/site-packages (from huggingface_hub>=0.25.0->peft) (1.2.0)\n",
      "Requirement already satisfied: setuptools in /Users/jyotirmoy/Desktop/Image/ancient-script-ai/ancient-ai-env/lib/python3.13/site-packages (from torch>=1.13.0->peft) (80.9.0)\n",
      "Requirement already satisfied: sympy>=1.13.3 in /Users/jyotirmoy/Desktop/Image/ancient-script-ai/ancient-ai-env/lib/python3.13/site-packages (from torch>=1.13.0->peft) (1.14.0)\n",
      "Requirement already satisfied: networkx>=2.5.1 in /Users/jyotirmoy/Desktop/Image/ancient-script-ai/ancient-ai-env/lib/python3.13/site-packages (from torch>=1.13.0->peft) (3.6)\n",
      "Requirement already satisfied: jinja2 in /Users/jyotirmoy/Desktop/Image/ancient-script-ai/ancient-ai-env/lib/python3.13/site-packages (from torch>=1.13.0->peft) (3.1.6)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /Users/jyotirmoy/Desktop/Image/ancient-script-ai/ancient-ai-env/lib/python3.13/site-packages (from sympy>=1.13.3->torch>=1.13.0->peft) (1.3.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /Users/jyotirmoy/Desktop/Image/ancient-script-ai/ancient-ai-env/lib/python3.13/site-packages (from jinja2->torch>=1.13.0->peft) (3.0.3)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in /Users/jyotirmoy/Desktop/Image/ancient-script-ai/ancient-ai-env/lib/python3.13/site-packages (from requests->huggingface_hub>=0.25.0->peft) (3.4.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /Users/jyotirmoy/Desktop/Image/ancient-script-ai/ancient-ai-env/lib/python3.13/site-packages (from requests->huggingface_hub>=0.25.0->peft) (3.11)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /Users/jyotirmoy/Desktop/Image/ancient-script-ai/ancient-ai-env/lib/python3.13/site-packages (from requests->huggingface_hub>=0.25.0->peft) (2.6.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /Users/jyotirmoy/Desktop/Image/ancient-script-ai/ancient-ai-env/lib/python3.13/site-packages (from requests->huggingface_hub>=0.25.0->peft) (2025.11.12)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /Users/jyotirmoy/Desktop/Image/ancient-script-ai/ancient-ai-env/lib/python3.13/site-packages (from transformers->peft) (2025.11.3)\n",
      "Requirement already satisfied: tokenizers<=0.23.0,>=0.22.0 in /Users/jyotirmoy/Desktop/Image/ancient-script-ai/ancient-ai-env/lib/python3.13/site-packages (from transformers->peft) (0.22.1)\n",
      "Using cached peft-0.18.0-py3-none-any.whl (556 kB)\n",
      "Using cached bitsandbytes-0.42.0-py3-none-any.whl (105.0 MB)\n",
      "Installing collected packages: bitsandbytes, peft\n",
      "\u001b[2K   \u001b[38;2;114;156;31m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m2/2\u001b[0m [peft]‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m1/2\u001b[0m [peft]\n",
      "\u001b[1A\u001b[2KSuccessfully installed bitsandbytes-0.42.0 peft-0.18.0\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install peft accelerate bitsandbytes\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e72b32df-d04e-44dd-aa05-09ed7dc5e7ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 1: Install (if needed) and imports\n",
    "\n",
    "# If peft is not installed, uncomment this:\n",
    "# %pip install peft accelerate\n",
    "\n",
    "from pathlib import Path\n",
    "import os\n",
    "\n",
    "import torch\n",
    "from datasets import Dataset, DatasetDict\n",
    "from transformers import (\n",
    "    AutoTokenizer,\n",
    "    AutoModelForSeq2SeqLM,\n",
    "    DataCollatorForSeq2Seq,\n",
    "    Seq2SeqTrainer,\n",
    "    Seq2SeqTrainingArguments,\n",
    ")\n",
    "\n",
    "from peft import LoraConfig, get_peft_model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27e58509-2473-4ba8-8b10-ea1aa1d9a125",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='mps')"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Cell 2: Paths and configuration\n",
    "import torch \n",
    "PROJECT_ROOT = Path.cwd().resolve().parents[0] if Path.cwd().name == \"notebooks\" else Path.cwd()\n",
    "DATA_ROOT = PROJECT_ROOT / \"data\" / \"corpora\"\n",
    "\n",
    "# We start with Sanskrit ‚Üí English fine-tuning (sa_en_itihasa)\n",
    "CORPUS_DIR = DATA_ROOT / \"sn_en_itihasa\"\n",
    "\n",
    "MODEL_NAME = \"facebook/nllb-200-distilled-600M\"\n",
    "OUTPUT_DIR = PROJECT_ROOT / \"models\" / \"nllb_sa_en_lora\"\n",
    "\n",
    "MAX_SOURCE_LEN = 128\n",
    "MAX_TARGET_LEN = 128\n",
    "\n",
    "BATCH_SIZE = 2   # keep small for MPS memory\n",
    "EPOCHS = 2\n",
    "LEARNING_RATE = 2e-5\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.backends.cuda.is_available() else \"cpu\")\n",
    "device\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "2821d47b-5baf-4303-8fdb-6d25f429d524",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['source_text', 'target_text'],\n",
       "        num_rows: 75161\n",
       "    })\n",
       "    validation: Dataset({\n",
       "        features: ['source_text', 'target_text'],\n",
       "        num_rows: 6148\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Cell 3: Load parallel data from sa_en_itihasa (train + dev)\n",
    "\n",
    "def load_parallel_split(corpus_dir: Path, split: str = \"train\"):\n",
    "    src_file = corpus_dir / f\"{split}.sn\"\n",
    "    tgt_file = corpus_dir / f\"{split}.en\"\n",
    "    \n",
    "    assert src_file.exists(), f\"Missing: {src_file}\"\n",
    "    assert tgt_file.exists(), f\"Missing: {tgt_file}\"\n",
    "    \n",
    "    with src_file.open(\"r\", encoding=\"utf-8\") as f_src, tgt_file.open(\"r\", encoding=\"utf-8\") as f_tgt:\n",
    "        src_lines = [line.strip() for line in f_src]\n",
    "        tgt_lines = [line.strip() for line in f_tgt]\n",
    "    \n",
    "    # Align lengths\n",
    "    n = min(len(src_lines), len(tgt_lines))\n",
    "    src_lines = src_lines[:n]\n",
    "    tgt_lines = tgt_lines[:n]\n",
    "    \n",
    "    return Dataset.from_dict({\n",
    "        \"source_text\": src_lines,\n",
    "        \"target_text\": tgt_lines,\n",
    "    })\n",
    "\n",
    "train_ds = load_parallel_split(CORPUS_DIR, \"train\")\n",
    "val_ds   = load_parallel_split(CORPUS_DIR, \"dev\")\n",
    "\n",
    "datasets = DatasetDict({\n",
    "    \"train\": train_ds,\n",
    "    \"validation\": val_ds\n",
    "})\n",
    "\n",
    "datasets\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "4afbf351-ecca-4581-803a-c1b86264722d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üîπ Loading NLLB base model...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "'(ReadTimeoutError(\"HTTPSConnectionPool(host='huggingface.co', port=443): Read timed out. (read timeout=10)\"), '(Request ID: 0d8ae635-bdad-45f3-b901-063a4312cc00)')' thrown while requesting HEAD https://huggingface.co/api/resolve-cache/models/facebook/nllb-200-distilled-600M/f8d333a098d19b4fd9a8b18f94170487ad3f821d/tokenizer_config.json\n",
      "Retrying in 1s [Retry 1/5].\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚öôÔ∏è  MPS device detected ‚Äî using Apple GPU for training.\n",
      "\n",
      "‚úÖ Model is ready for fine-tuning.\n",
      "trainable params: 1,179,648 || all params: 616,253,440 || trainable%: 0.1914\n"
     ]
    }
   ],
   "source": [
    "# Cell 4: Load NLLB model and tokenizer, prepare LoRA (optimized for MPS GPU)\n",
    "\n",
    "print(\"üîπ Loading NLLB base model...\")\n",
    "tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)\n",
    "model = AutoModelForSeq2SeqLM.from_pretrained(MODEL_NAME)\n",
    "\n",
    "# -------------------------------\n",
    "# Apple Silicon (MPS) configuration\n",
    "# -------------------------------\n",
    "if torch.backends.mps.is_available():\n",
    "    device = torch.device(\"mps\")\n",
    "    print(\"‚öôÔ∏è  MPS device detected ‚Äî using Apple GPU for training.\")\n",
    "else:\n",
    "    device = torch.device(\"cpu\")\n",
    "    print(\"‚ö†Ô∏è  MPS not available. Falling back to CPU.\")\n",
    "\n",
    "model = model.to(device)\n",
    "\n",
    "# Configure LoRA (Low-Rank Adapters)\n",
    "lora_config = LoraConfig(\n",
    "    task_type=\"SEQ_2_SEQ_LM\",\n",
    "    r=8,               # LoRA rank\n",
    "    lora_alpha=32,     # scaling factor\n",
    "    lora_dropout=0.1,  # regularization\n",
    "    target_modules=[\"q_proj\", \"v_proj\"]  # attention projection layers\n",
    ")\n",
    "\n",
    "# Wrap model with PEFT (LoRA)\n",
    "model = get_peft_model(model, lora_config)\n",
    "\n",
    "# Confirm trainable parameters (should be only LoRA matrices)\n",
    "print(\"\\n‚úÖ Model is ready for fine-tuning.\")\n",
    "model.print_trainable_parameters()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "f665868d-089d-479b-91d4-4299c67e600c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f75cd063f6a34b1a8dcae106cafccd69",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/75161 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "84291a5dee7e4752a1b456faa8b06839",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/6148 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['input_ids', 'attention_mask', 'labels'],\n",
       "        num_rows: 75161\n",
       "    })\n",
       "    validation: Dataset({\n",
       "        features: ['input_ids', 'attention_mask', 'labels'],\n",
       "        num_rows: 6148\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Cell 5: Preprocessing function (modern Hugging Face API)\n",
    "# Sanskrit ‚Üí English\n",
    "\n",
    "SRC_LANG_CODE = \"san_Deva\"\n",
    "TGT_LANG_CODE = \"eng_Latn\"\n",
    "\n",
    "def preprocess_function(examples):\n",
    "    tokenizer.src_lang = SRC_LANG_CODE\n",
    "    tokenizer.tgt_lang = TGT_LANG_CODE\n",
    "\n",
    "    # Tokenize both source and target in one go\n",
    "    model_inputs = tokenizer(\n",
    "        examples[\"source_text\"],\n",
    "        text_target=examples[\"target_text\"],\n",
    "        max_length=MAX_SOURCE_LEN,\n",
    "        truncation=True,\n",
    "        padding=\"max_length\",\n",
    "    )\n",
    "\n",
    "    return model_inputs\n",
    "\n",
    "# Map the function to your dataset\n",
    "tokenized_datasets = datasets.map(\n",
    "    preprocess_function,\n",
    "    batched=True,\n",
    "    remove_columns=[\"source_text\", \"target_text\"]\n",
    ")\n",
    "\n",
    "tokenized_datasets\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "67595d5f-2376-4540-b385-019ec4efc858",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 6: Data collator for seq2seq\n",
    "\n",
    "data_collator = DataCollatorForSeq2Seq(tokenizer, model=model)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "93753937-53e6-44b3-9115-d7fa829da1e5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Seq2SeqTrainingArguments(output_dir='/Users/jyotirmoy/Desktop/Image/ancient-script-ai/logs/nllb_sa_en_lora', overwrite_output_dir=False, do_train=False, do_eval=True, do_predict=False, eval_strategy=<IntervalStrategy.EPOCH: 'epoch'>, prediction_loss_only=False, per_device_train_batch_size=2, per_device_eval_batch_size=2, per_gpu_train_batch_size=None, per_gpu_eval_batch_size=None, gradient_accumulation_steps=1, eval_accumulation_steps=None, eval_delay=0, torch_empty_cache_steps=None, learning_rate=2e-05, weight_decay=0.01, adam_beta1=0.9, adam_beta2=0.999, adam_epsilon=1e-08, max_grad_norm=1.0, num_train_epochs=2, max_steps=-1, lr_scheduler_type=<SchedulerType.LINEAR: 'linear'>, lr_scheduler_kwargs={}, warmup_ratio=0.0, warmup_steps=0, log_level='passive', log_level_replica='warning', log_on_each_node=True, logging_dir='/Users/jyotirmoy/Desktop/Image/ancient-script-ai/logs/nllb_sa_en_lora', logging_strategy=<IntervalStrategy.STEPS: 'steps'>, logging_first_step=False, logging_steps=50, logging_nan_inf_filter=True, save_strategy=<SaveStrategy.EPOCH: 'epoch'>, save_steps=500, save_total_limit=2, save_safetensors=True, save_on_each_node=False, save_only_model=False, restore_callback_states_from_checkpoint=False, no_cuda=False, use_cpu=False, use_mps_device=False, seed=42, data_seed=None, jit_mode_eval=False, bf16=False, fp16=False, fp16_opt_level='O1', half_precision_backend='auto', bf16_full_eval=False, fp16_full_eval=False, tf32=None, local_rank=0, ddp_backend=None, tpu_num_cores=None, tpu_metrics_debug=False, debug=[], dataloader_drop_last=False, eval_steps=None, dataloader_num_workers=0, dataloader_prefetch_factor=None, past_index=-1, run_name=None, disable_tqdm=False, remove_unused_columns=True, label_names=None, load_best_model_at_end=False, metric_for_best_model=None, greater_is_better=None, ignore_data_skip=False, fsdp=[], fsdp_min_num_params=0, fsdp_config={'min_num_params': 0, 'xla': False, 'xla_fsdp_v2': False, 'xla_fsdp_grad_ckpt': False}, fsdp_transformer_layer_cls_to_wrap=None, accelerator_config=AcceleratorConfig(split_batches=False, dispatch_batches=None, even_batches=True, use_seedable_sampler=True, non_blocking=False, gradient_accumulation_kwargs=None, use_configured_state=False), parallelism_config=None, deepspeed=None, label_smoothing_factor=0.0, optim=<OptimizerNames.ADAMW_TORCH_FUSED: 'adamw_torch_fused'>, optim_args=None, adafactor=False, group_by_length=False, length_column_name='length', report_to=[], project='huggingface', trackio_space_id='trackio', ddp_find_unused_parameters=None, ddp_bucket_cap_mb=None, ddp_broadcast_buffers=None, dataloader_pin_memory=True, dataloader_persistent_workers=False, skip_memory_metrics=True, use_legacy_prediction_loop=False, push_to_hub=False, resume_from_checkpoint=None, hub_model_id=None, hub_strategy=<HubStrategy.EVERY_SAVE: 'every_save'>, hub_token=None, hub_private_repo=None, hub_always_push=False, hub_revision=None, gradient_checkpointing=False, gradient_checkpointing_kwargs=None, include_inputs_for_metrics=False, include_for_metrics=[], eval_do_concat_batches=True, fp16_backend='auto', push_to_hub_model_id=None, push_to_hub_organization=None, push_to_hub_token=None, mp_parameters='', auto_find_batch_size=False, full_determinism=False, torchdynamo=None, ray_scope='last', ddp_timeout=1800, torch_compile=False, torch_compile_backend=None, torch_compile_mode=None, include_tokens_per_second=False, include_num_input_tokens_seen='no', neftune_noise_alpha=None, optim_target_modules=None, batch_eval_metrics=False, eval_on_start=False, use_liger_kernel=False, liger_kernel_config=None, eval_use_gather_object=False, average_tokens_across_devices=True, sortish_sampler=False, predict_with_generate=True, generation_max_length=None, generation_num_beams=None, generation_config=None)"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers import Seq2SeqTrainingArguments\n",
    "\n",
    "# Cell 7: Training configuration (compatible across new transformers versions)\n",
    "\n",
    "training_args = Seq2SeqTrainingArguments(\n",
    "    output_dir=str(PROJECT_ROOT / \"logs\" / \"nllb_sa_en_lora\"),\n",
    "    eval_strategy=\"epoch\",               # <- fixed key for newer versions\n",
    "    save_strategy=\"epoch\",\n",
    "    learning_rate=LEARNING_RATE,\n",
    "    per_device_train_batch_size=BATCH_SIZE,\n",
    "    per_device_eval_batch_size=BATCH_SIZE,\n",
    "    num_train_epochs=EPOCHS,\n",
    "    weight_decay=0.01,\n",
    "    predict_with_generate=True,\n",
    "    logging_dir=str(PROJECT_ROOT / \"logs\" / \"nllb_sa_en_lora\"),\n",
    "    logging_steps=50,\n",
    "    save_total_limit=2,\n",
    "    report_to=[],                        # replaces \"none\"\n",
    ")\n",
    "\n",
    "training_args\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "666613de-05e7-43bf-8255-b25410ca0464",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚öôÔ∏è Using device: mps\n",
      "Using 4000 train examples and 400 validation examples.\n",
      "üöÄ Starting LoRA fine-tuning for Sanskrit ‚Üí English on Apple MPS GPU...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/p7/h3z23h2127x2jrspfn8fy4h40000gn/T/ipykernel_1852/452309910.py:33: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `MPSSeq2SeqTrainer.__init__`. Use `processing_class` instead.\n",
      "  trainer = MPSSeq2SeqTrainer(\n"
     ]
    },
    {
     "ename": "RecursionError",
     "evalue": "maximum recursion depth exceeded",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mRecursionError\u001b[39m                            Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[20]\u001b[39m\u001b[32m, line 43\u001b[39m\n\u001b[32m     33\u001b[39m trainer = MPSSeq2SeqTrainer(\n\u001b[32m     34\u001b[39m     model=model,\n\u001b[32m     35\u001b[39m     args=training_args,           \u001b[38;5;66;03m# same TrainingArguments from earlier cell\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m     39\u001b[39m     data_collator=data_collator,\n\u001b[32m     40\u001b[39m )\n\u001b[32m     42\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33müöÄ Starting LoRA fine-tuning for Sanskrit ‚Üí English on Apple MPS GPU...\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m---> \u001b[39m\u001b[32m43\u001b[39m train_result = \u001b[43mtrainer\u001b[49m\u001b[43m.\u001b[49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     44\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33m‚úÖ Fine-tuning complete.\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m     46\u001b[39m \u001b[38;5;66;03m# Save just the LoRA fine-tuned weights\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/Image/ancient-script-ai/ancient-ai-env/lib/python3.13/site-packages/transformers/trainer.py:2325\u001b[39m, in \u001b[36mTrainer.train\u001b[39m\u001b[34m(self, resume_from_checkpoint, trial, ignore_keys_for_eval, **kwargs)\u001b[39m\n\u001b[32m   2323\u001b[39m         hf_hub_utils.enable_progress_bars()\n\u001b[32m   2324\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m2325\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43minner_training_loop\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   2326\u001b[39m \u001b[43m        \u001b[49m\u001b[43margs\u001b[49m\u001b[43m=\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2327\u001b[39m \u001b[43m        \u001b[49m\u001b[43mresume_from_checkpoint\u001b[49m\u001b[43m=\u001b[49m\u001b[43mresume_from_checkpoint\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2328\u001b[39m \u001b[43m        \u001b[49m\u001b[43mtrial\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtrial\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2329\u001b[39m \u001b[43m        \u001b[49m\u001b[43mignore_keys_for_eval\u001b[49m\u001b[43m=\u001b[49m\u001b[43mignore_keys_for_eval\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2330\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/Image/ancient-script-ai/ancient-ai-env/lib/python3.13/site-packages/transformers/trainer.py:2674\u001b[39m, in \u001b[36mTrainer._inner_training_loop\u001b[39m\u001b[34m(self, batch_size, args, resume_from_checkpoint, trial, ignore_keys_for_eval)\u001b[39m\n\u001b[32m   2667\u001b[39m context = (\n\u001b[32m   2668\u001b[39m     functools.partial(\u001b[38;5;28mself\u001b[39m.accelerator.no_sync, model=model)\n\u001b[32m   2669\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m i != \u001b[38;5;28mlen\u001b[39m(batch_samples) - \u001b[32m1\u001b[39m\n\u001b[32m   2670\u001b[39m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m.accelerator.distributed_type != DistributedType.DEEPSPEED\n\u001b[32m   2671\u001b[39m     \u001b[38;5;28;01melse\u001b[39;00m contextlib.nullcontext\n\u001b[32m   2672\u001b[39m )\n\u001b[32m   2673\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m context():\n\u001b[32m-> \u001b[39m\u001b[32m2674\u001b[39m     tr_loss_step = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mtraining_step\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_items_in_batch\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   2676\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[32m   2677\u001b[39m     args.logging_nan_inf_filter\n\u001b[32m   2678\u001b[39m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m is_torch_xla_available()\n\u001b[32m   2679\u001b[39m     \u001b[38;5;129;01mand\u001b[39;00m (torch.isnan(tr_loss_step) \u001b[38;5;129;01mor\u001b[39;00m torch.isinf(tr_loss_step))\n\u001b[32m   2680\u001b[39m ):\n\u001b[32m   2681\u001b[39m     \u001b[38;5;66;03m# if loss is nan or inf simply add the average of previous logged losses\u001b[39;00m\n\u001b[32m   2682\u001b[39m     tr_loss = tr_loss + tr_loss / (\u001b[32m1\u001b[39m + \u001b[38;5;28mself\u001b[39m.state.global_step - \u001b[38;5;28mself\u001b[39m._globalstep_last_logged)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/Image/ancient-script-ai/ancient-ai-env/lib/python3.13/site-packages/transformers/trainer.py:4020\u001b[39m, in \u001b[36mTrainer.training_step\u001b[39m\u001b[34m(self, model, inputs, num_items_in_batch)\u001b[39m\n\u001b[32m   4017\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m loss_mb.reduce_mean().detach().to(\u001b[38;5;28mself\u001b[39m.args.device)\n\u001b[32m   4019\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m.compute_loss_context_manager():\n\u001b[32m-> \u001b[39m\u001b[32m4020\u001b[39m     loss = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mcompute_loss\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_items_in_batch\u001b[49m\u001b[43m=\u001b[49m\u001b[43mnum_items_in_batch\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   4022\u001b[39m \u001b[38;5;28;01mdel\u001b[39;00m inputs\n\u001b[32m   4023\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[32m   4024\u001b[39m     \u001b[38;5;28mself\u001b[39m.args.torch_empty_cache_steps \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   4025\u001b[39m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m.state.global_step % \u001b[38;5;28mself\u001b[39m.args.torch_empty_cache_steps == \u001b[32m0\u001b[39m\n\u001b[32m   4026\u001b[39m ):\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[20]\u001b[39m\u001b[32m, line 23\u001b[39m, in \u001b[36mMPSSeq2SeqTrainer.compute_loss\u001b[39m\u001b[34m(self, model, inputs, return_outputs, num_items_in_batch)\u001b[39m\n\u001b[32m     15\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mcompute_loss\u001b[39m(\n\u001b[32m     16\u001b[39m     \u001b[38;5;28mself\u001b[39m,\n\u001b[32m     17\u001b[39m     model,\n\u001b[32m   (...)\u001b[39m\u001b[32m     21\u001b[39m ):\n\u001b[32m     22\u001b[39m     \u001b[38;5;66;03m# DO NOT forward num_items_in_batch to super() or model\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m23\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m.\u001b[49m\u001b[43mcompute_loss\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mreturn_outputs\u001b[49m\u001b[43m=\u001b[49m\u001b[43mreturn_outputs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[9]\u001b[39m\u001b[32m, line 22\u001b[39m, in \u001b[36msafe_compute_loss\u001b[39m\u001b[34m(self, model, inputs, return_outputs, num_items_in_batch)\u001b[39m\n\u001b[32m     19\u001b[39m \u001b[38;5;66;03m# Move all tensors in inputs to the correct device (MPS)\u001b[39;00m\n\u001b[32m     20\u001b[39m inputs = {k: v.to(device) \u001b[38;5;28;01mif\u001b[39;00m torch.is_tensor(v) \u001b[38;5;28;01melse\u001b[39;00m v \u001b[38;5;28;01mfor\u001b[39;00m k, v \u001b[38;5;129;01min\u001b[39;00m inputs.items()}\n\u001b[32m---> \u001b[39m\u001b[32m22\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43morig_compute_loss\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mreturn_outputs\u001b[49m\u001b[43m=\u001b[49m\u001b[43mreturn_outputs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[9]\u001b[39m\u001b[32m, line 22\u001b[39m, in \u001b[36msafe_compute_loss\u001b[39m\u001b[34m(self, model, inputs, return_outputs, num_items_in_batch)\u001b[39m\n\u001b[32m     19\u001b[39m \u001b[38;5;66;03m# Move all tensors in inputs to the correct device (MPS)\u001b[39;00m\n\u001b[32m     20\u001b[39m inputs = {k: v.to(device) \u001b[38;5;28;01mif\u001b[39;00m torch.is_tensor(v) \u001b[38;5;28;01melse\u001b[39;00m v \u001b[38;5;28;01mfor\u001b[39;00m k, v \u001b[38;5;129;01min\u001b[39;00m inputs.items()}\n\u001b[32m---> \u001b[39m\u001b[32m22\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43morig_compute_loss\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mreturn_outputs\u001b[49m\u001b[43m=\u001b[49m\u001b[43mreturn_outputs\u001b[49m\u001b[43m)\u001b[49m\n",
      "    \u001b[31m[... skipping similar frames: safe_compute_loss at line 22 (2970 times)]\u001b[39m\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[9]\u001b[39m\u001b[32m, line 22\u001b[39m, in \u001b[36msafe_compute_loss\u001b[39m\u001b[34m(self, model, inputs, return_outputs, num_items_in_batch)\u001b[39m\n\u001b[32m     19\u001b[39m \u001b[38;5;66;03m# Move all tensors in inputs to the correct device (MPS)\u001b[39;00m\n\u001b[32m     20\u001b[39m inputs = {k: v.to(device) \u001b[38;5;28;01mif\u001b[39;00m torch.is_tensor(v) \u001b[38;5;28;01melse\u001b[39;00m v \u001b[38;5;28;01mfor\u001b[39;00m k, v \u001b[38;5;129;01min\u001b[39;00m inputs.items()}\n\u001b[32m---> \u001b[39m\u001b[32m22\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43morig_compute_loss\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mreturn_outputs\u001b[49m\u001b[43m=\u001b[49m\u001b[43mreturn_outputs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[9]\u001b[39m\u001b[32m, line 20\u001b[39m, in \u001b[36msafe_compute_loss\u001b[39m\u001b[34m(self, model, inputs, return_outputs, num_items_in_batch)\u001b[39m\n\u001b[32m     17\u001b[39m     \u001b[38;5;28;01mdel\u001b[39;00m inputs[\u001b[33m\"\u001b[39m\u001b[33mnum_items_in_batch\u001b[39m\u001b[33m\"\u001b[39m]\n\u001b[32m     19\u001b[39m \u001b[38;5;66;03m# Move all tensors in inputs to the correct device (MPS)\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m20\u001b[39m inputs = {k: v.to(device) \u001b[38;5;28;01mif\u001b[39;00m \u001b[43mtorch\u001b[49m\u001b[43m.\u001b[49m\u001b[43mis_tensor\u001b[49m\u001b[43m(\u001b[49m\u001b[43mv\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;28;01melse\u001b[39;00m v \u001b[38;5;28;01mfor\u001b[39;00m k, v \u001b[38;5;129;01min\u001b[39;00m inputs.items()}\n\u001b[32m     22\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m orig_compute_loss(\u001b[38;5;28mself\u001b[39m, model, inputs, return_outputs=return_outputs)\n",
      "\u001b[31mRecursionError\u001b[39m: maximum recursion depth exceeded"
     ]
    }
   ],
   "source": [
    "# ============================================================\n",
    "# Cell 8: Fast LoRA fine-tuning on MPS with safe Trainer\n",
    "# ============================================================\n",
    "\n",
    "import torch\n",
    "from transformers import Seq2SeqTrainer\n",
    "\n",
    "# Make sure model is on MPS (Apple GPU)\n",
    "device = torch.device(\"cuda\" if torch.backends.cuda.is_available() else \"cpu\")\n",
    "model.to(device)\n",
    "print(f\"‚öôÔ∏è Using device: {device}\")\n",
    "\n",
    "# ---- Custom Trainer that ignores `num_items_in_batch` ----\n",
    "class MPSSeq2SeqTrainer(Seq2SeqTrainer):\n",
    "    def compute_loss(\n",
    "        self,\n",
    "        model,\n",
    "        inputs,\n",
    "        return_outputs: bool = False,\n",
    "        num_items_in_batch: int | None = None,   # Trainer will pass this, we just ignore it\n",
    "    ):\n",
    "        # DO NOT forward num_items_in_batch to super() or model\n",
    "        return super().compute_loss(model, inputs, return_outputs=return_outputs)\n",
    "\n",
    "# ---- Use a smaller subset for faster experiments ----\n",
    "# You can increase these numbers later once you're happy.\n",
    "train_subset = tokenized_datasets[\"train\"].select(range(4000))       # e.g. first 4k examples\n",
    "val_subset   = tokenized_datasets[\"validation\"].select(range(400))   # first 400 examples\n",
    "\n",
    "print(f\"Using {len(train_subset)} train examples and {len(val_subset)} validation examples.\")\n",
    "\n",
    "# ---- Define Trainer (no monkey patching) ----\n",
    "trainer = MPSSeq2SeqTrainer(\n",
    "    model=model,\n",
    "    args=training_args,           # same TrainingArguments from earlier cell\n",
    "    train_dataset=train_subset,\n",
    "    eval_dataset=val_subset,\n",
    "    tokenizer=tokenizer,\n",
    "    data_collator=data_collator,\n",
    ")\n",
    "\n",
    "print(\"üöÄ Starting LoRA fine-tuning for Sanskrit ‚Üí English on Apple MPS GPU...\")\n",
    "train_result = trainer.train()\n",
    "print(\"‚úÖ Fine-tuning complete.\")\n",
    "\n",
    "# Save just the LoRA fine-tuned weights\n",
    "output_path = \"models/nllb_sa_en_lora_fast.pth\"\n",
    "torch.save(model.state_dict(), output_path)\n",
    "print(f\"üíæ Saved LoRA weights to: {output_path}\")\n",
    "train_result\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e826df76-315a-4f15-9848-aab3700354f2",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (Ancient-AI)",
   "language": "python",
   "name": "ancient-ai-env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
