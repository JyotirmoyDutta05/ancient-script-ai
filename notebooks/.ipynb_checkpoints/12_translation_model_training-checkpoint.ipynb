{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "e5498613-50f3-490c-8334-23266e214979",
   "metadata": {},
   "outputs": [],
   "source": [
    "# If these are already installed in ancient-ai-env, you can skip this cell.\n",
    "# Otherwise run it once.\n",
    "\n",
    "try:\n",
    "    import datasets  # type: ignore\n",
    "except ImportError:\n",
    "    %pip install datasets --quiet\n",
    "\n",
    "try:\n",
    "    import transformers  # type: ignore\n",
    "except ImportError:\n",
    "    %pip install transformers accelerate sentencepiece --quiet\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "011ce184-604c-487f-b758-438f864d1a55",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Project root : /Users/jyotirmoy/Desktop/Image/ancient-script-ai\n",
      "Corpus dir   : /Users/jyotirmoy/Desktop/Image/ancient-script-ai/data/corpora/sa_en_itihasa exists\n",
      "Model dir    : /Users/jyotirmoy/Desktop/Image/ancient-script-ai/models/indictrans2-en exists\n",
      "Log dir      : /Users/jyotirmoy/Desktop/Image/ancient-script-ai/logs/translation_finetune\n"
     ]
    }
   ],
   "source": [
    "from pathlib import Path\n",
    "import unicodedata\n",
    "import re\n",
    "\n",
    "import pandas as pd\n",
    "from datasets import Dataset, DatasetDict\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "from transformers import (\n",
    "    AutoTokenizer,\n",
    "    AutoModelForSeq2SeqLM,\n",
    "    DataCollatorForSeq2Seq,\n",
    "    Trainer,\n",
    "    TrainingArguments,\n",
    ")\n",
    "\n",
    "BASE_DIR = Path(\"..\").resolve()\n",
    "DATA_DIR = BASE_DIR / \"data\" / \"corpora\" / \"sa_en_itihasa\"\n",
    "MODEL_DIR = BASE_DIR / \"models\" / \"indictrans2-en\"\n",
    "LOG_DIR = BASE_DIR / \"logs\" / \"translation_finetune\"\n",
    "LOG_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "print(\"Project root :\", BASE_DIR)\n",
    "print(\"Corpus dir   :\", DATA_DIR, \"exists\" if DATA_DIR.exists() else \"MISSING\")\n",
    "print(\"Model dir    :\", MODEL_DIR, \"exists\" if MODEL_DIR.exists() else \"MISSING\")\n",
    "print(\"Log dir      :\", LOG_DIR)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "7d2939e2-fcb7-4edf-9514-86bbbac22981",
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize_text(text: str) -> str:\n",
    "    \"\"\"\n",
    "    Basic Unicode normalization and cleanup for Sanskrit / English text.\n",
    "    - NFC normalization\n",
    "    - remove zero-width characters\n",
    "    - strip whitespace\n",
    "    \"\"\"\n",
    "    if not isinstance(text, str):\n",
    "        return \"\"\n",
    "    text = unicodedata.normalize(\"NFC\", text)\n",
    "    text = re.sub(r\"[\\u200b\\u200c\\u200d]\", \"\", text)  # zero-width chars\n",
    "    text = text.strip()\n",
    "    return text\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "93267e07-189e-42c9-b8cf-e2acd4096b54",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total sentence pairs: 75161\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>src_sa</th>\n",
       "      <th>tgt_en</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>ॐ तपः स्वाध्यायनिरतं तपस्वी वाग्विदां वरम्। ना...</td>\n",
       "      <td>The ascetic Vālmīki asked Nārada, the best of ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>कोन्वस्मिन् साम्प्रतं लोके गुणवान् कश्च वीर्यव...</td>\n",
       "      <td>Who at present in this world is like crowned w...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>चारित्रेण च को युक्तः सर्वभूतेषु को हितः। विद्...</td>\n",
       "      <td>Who is qualified by virtue of his character, a...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>आत्मवान् को जितक्रोधो द्युतिमान् कोऽनसूयकः। कस...</td>\n",
       "      <td>Who has subdued his heart, and controlled his ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>एतदिच्छाम्यहं श्रोतुं परं कौतूहलं हि मे। महर्ष...</td>\n",
       "      <td>I have great curiosity to hear of such a perso...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                              src_sa  \\\n",
       "0  ॐ तपः स्वाध्यायनिरतं तपस्वी वाग्विदां वरम्। ना...   \n",
       "1  कोन्वस्मिन् साम्प्रतं लोके गुणवान् कश्च वीर्यव...   \n",
       "2  चारित्रेण च को युक्तः सर्वभूतेषु को हितः। विद्...   \n",
       "3  आत्मवान् को जितक्रोधो द्युतिमान् कोऽनसूयकः। कस...   \n",
       "4  एतदिच्छाम्यहं श्रोतुं परं कौतूहलं हि मे। महर्ष...   \n",
       "\n",
       "                                              tgt_en  \n",
       "0  The ascetic Vālmīki asked Nārada, the best of ...  \n",
       "1  Who at present in this world is like crowned w...  \n",
       "2  Who is qualified by virtue of his character, a...  \n",
       "3  Who has subdued his heart, and controlled his ...  \n",
       "4  I have great curiosity to hear of such a perso...  "
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_en_path = DATA_DIR / \"train.en\"\n",
    "train_sa_path = DATA_DIR / \"train.sn\"\n",
    "\n",
    "if not train_en_path.exists() or not train_sa_path.exists():\n",
    "    raise FileNotFoundError(\n",
    "        f\"Expected files train.en and train.sn inside {DATA_DIR}, \"\n",
    "        f\"found: train.en={train_en_path.exists()}, train.sn={train_sa_path.exists()}\"\n",
    "    )\n",
    "\n",
    "sanskrit_lines = [normalize_text(l) for l in train_sa_path.read_text(encoding=\"utf-8\").splitlines()]\n",
    "english_lines  = [normalize_text(l) for l in train_en_path.read_text(encoding=\"utf-8\").splitlines()]\n",
    "\n",
    "if len(sanskrit_lines) != len(english_lines):\n",
    "    raise ValueError(f\"Line mismatch: Sanskrit={len(sanskrit_lines)}, English={len(english_lines)}\")\n",
    "\n",
    "df = pd.DataFrame({\"src_sa\": sanskrit_lines, \"tgt_en\": english_lines})\n",
    "print(\"Total sentence pairs:\", len(df))\n",
    "df.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "af4dd42c-d93e-40ae-9bc4-8a6622c639b8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train size: 67644\n",
      "Val size  : 7517\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>src_sa</th>\n",
       "      <th>tgt_en</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>41469</th>\n",
       "      <td>नागं जिघांसुः सहसा चिक्षेप च महाबलः। स विस्फुल...</td>\n",
       "      <td>Then that highly powerful hero desirous of sla...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36243</th>\n",
       "      <td>प्रोक्षिता यत्र बहवो वराहाद्या मृगा वने। शक्रे...</td>\n",
       "      <td>Here were massacred many boars and Other anima...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39166</th>\n",
       "      <td>इहैव तैर्जितः सर्गो येषां साम्ये स्थितं मनः। न...</td>\n",
       "      <td>Even here the material world is conquered by t...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>48921</th>\n",
       "      <td>धनंजयस्ततः कृष्णमब्रवीत् पश्य केशव। आचार्यरथमु...</td>\n",
       "      <td>Thereafter Dhananjaya addressing Kesava said-\"...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>40873</th>\n",
       "      <td>क्रुद्धं तमुवीक्ष्य भयेन राजन् सम्मूर्च्छितो न...</td>\n",
       "      <td>Beholding Bhima, I have been, O sire, unmanned...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                  src_sa  \\\n",
       "41469  नागं जिघांसुः सहसा चिक्षेप च महाबलः। स विस्फुल...   \n",
       "36243  प्रोक्षिता यत्र बहवो वराहाद्या मृगा वने। शक्रे...   \n",
       "39166  इहैव तैर्जितः सर्गो येषां साम्ये स्थितं मनः। न...   \n",
       "48921  धनंजयस्ततः कृष्णमब्रवीत् पश्य केशव। आचार्यरथमु...   \n",
       "40873  क्रुद्धं तमुवीक्ष्य भयेन राजन् सम्मूर्च्छितो न...   \n",
       "\n",
       "                                                  tgt_en  \n",
       "41469  Then that highly powerful hero desirous of sla...  \n",
       "36243  Here were massacred many boars and Other anima...  \n",
       "39166  Even here the material world is conquered by t...  \n",
       "48921  Thereafter Dhananjaya addressing Kesava said-\"...  \n",
       "40873  Beholding Bhima, I have been, O sire, unmanned...  "
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df, val_df = train_test_split(df, test_size=0.1, random_state=42)\n",
    "\n",
    "print(\"Train size:\", len(train_df))\n",
    "print(\"Val size  :\", len(val_df))\n",
    "\n",
    "train_df.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "f41daa1d-064e-4dac-acb9-7e56469710b2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['src_sa', 'tgt_en'],\n",
       "        num_rows: 67644\n",
       "    })\n",
       "    validation: Dataset({\n",
       "        features: ['src_sa', 'tgt_en'],\n",
       "        num_rows: 7517\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_ds = Dataset.from_pandas(train_df.reset_index(drop=True))\n",
    "val_ds   = Dataset.from_pandas(val_df.reset_index(drop=True))\n",
    "\n",
    "raw_datasets = DatasetDict(\n",
    "    {\n",
    "        \"train\": train_ds,\n",
    "        \"validation\": val_ds,\n",
    "    }\n",
    ")\n",
    "\n",
    "raw_datasets\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "0180e841-7d1e-443e-bf40-72dd4d12929a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "The repository /Users/jyotirmoy/Desktop/Image/ancient-script-ai/models/indictrans2-en contains custom code which must be executed to correctly load the model. You can inspect the repository content at /Users/jyotirmoy/Desktop/Image/ancient-script-ai/models/indictrans2-en .\n",
      " You can inspect the repository content at https://hf.co//Users/jyotirmoy/Desktop/Image/ancient-script-ai/models/indictrans2-en.\n",
      "You can avoid this prompt in future by passing the argument `trust_remote_code=True`.\n",
      "\n",
      "Do you wish to run the custom code? [y/N]  y\n",
      "The repository /Users/jyotirmoy/Desktop/Image/ancient-script-ai/models/indictrans2-en contains custom code which must be executed to correctly load the model. You can inspect the repository content at /Users/jyotirmoy/Desktop/Image/ancient-script-ai/models/indictrans2-en .\n",
      " You can inspect the repository content at https://hf.co//Users/jyotirmoy/Desktop/Image/ancient-script-ai/models/indictrans2-en.\n",
      "You can avoid this prompt in future by passing the argument `trust_remote_code=True`.\n",
      "\n",
      "Do you wish to run the custom code? [y/N]  y\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokenizer vocab size: 122706\n",
      "Model loaded: <class 'transformers_modules.indictrans2_hyphen_en.modeling_indictrans.IndicTransForConditionalGeneration'>\n"
     ]
    }
   ],
   "source": [
    "if not MODEL_DIR.exists():\n",
    "    raise FileNotFoundError(f\"Model directory not found: {MODEL_DIR}\")\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(str(MODEL_DIR), use_fast=True)\n",
    "model = AutoModelForSeq2SeqLM.from_pretrained(str(MODEL_DIR))\n",
    "\n",
    "print(\"Tokenizer vocab size:\", tokenizer.vocab_size)\n",
    "print(\"Model loaded:\", type(model))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "92841c00-ab5a-4fb4-97bf-6065c6fb9dc7",
   "metadata": {},
   "outputs": [],
   "source": [
    "max_source_length = 256\n",
    "max_target_length = 256\n",
    "\n",
    "SRC_LANG_TAG = \"<2sa>\"   # Sanskrit source\n",
    "TGT_LANG_TAG = \"<2en>\"   # English target\n",
    "\n",
    "def preprocess_function(examples):\n",
    "    \"\"\"\n",
    "    Tokenize Sanskrit source and English target text.\n",
    "    Adds required IndicTrans2 language tags to each example.\n",
    "    \"\"\"\n",
    "    # add language tags\n",
    "    inputs = [f\"{SRC_LANG_TAG} {TGT_LANG_TAG} {text}\" for text in examples[\"src_sa\"]]\n",
    "    targets = [f\"{TGT_LANG_TAG} {text}\" for text in examples[\"tgt_en\"]]\n",
    "\n",
    "    model_inputs = tokenizer(\n",
    "        inputs,\n",
    "        max_length=max_source_length,\n",
    "        truncation=True,\n",
    "    )\n",
    "\n",
    "    # Tokenize targets separately\n",
    "    with tokenizer.as_target_tokenizer():\n",
    "        labels = tokenizer(\n",
    "            targets,\n",
    "            max_length=max_target_length,\n",
    "            truncation=True,\n",
    "        )\n",
    "\n",
    "    model_inputs[\"labels\"] = labels[\"input_ids\"]\n",
    "    return model_inputs\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "f021bad2-5381-4a1a-86d4-9871eaa9e9c3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1acd76008d4b4aa3abf55bbbc16aad72",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/67644 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/jyotirmoy/Desktop/Image/ancient-script-ai/ancient-ai-env/lib/python3.13/site-packages/transformers/tokenization_utils_base.py:4169: UserWarning: `as_target_tokenizer` is deprecated and will be removed in v5 of Transformers. You can tokenize your labels by using the argument `text_target` of the regular `__call__` method (either in the same call as your input texts if you use the same keyword arguments, or in a separate call.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8bd87c0459954567b3eb5e5c543e2d97",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/7517 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<unk><unk></s>\n"
     ]
    }
   ],
   "source": [
    "# Re-map with corrected preprocessing\n",
    "tokenized_datasets = raw_datasets.map(\n",
    "    preprocess_function,\n",
    "    batched=True,\n",
    "    remove_columns=raw_datasets[\"train\"].column_names,\n",
    ")\n",
    "\n",
    "# sanity check again\n",
    "example = tokenized_datasets[\"train\"][0]\n",
    "print(tokenizer.decode(example[\"input_ids\"], skip_special_tokens=False))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "1dad23c3-f211-442b-bc32-d4567b0cefbc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7d19044f24184f17819cd9750b0d0454",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/67644 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "958de115e9e443e0899b9dc5efe53bb2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/7517 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['input_ids', 'attention_mask', 'labels'],\n",
       "        num_rows: 67644\n",
       "    })\n",
       "    validation: Dataset({\n",
       "        features: ['input_ids', 'attention_mask', 'labels'],\n",
       "        num_rows: 7517\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenized_datasets = raw_datasets.map(\n",
    "    preprocess_function,\n",
    "    batched=True,\n",
    "    remove_columns=raw_datasets[\"train\"].column_names,\n",
    ")\n",
    "\n",
    "tokenized_datasets\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "44cebc1b-bcf0-4699-898f-9bc9ac90bcea",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_collator = DataCollatorForSeq2Seq(tokenizer=tokenizer, model=model)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "5797adf9-a3d4-48b3-bcd4-2e8dac749b44",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Keys: dict_keys(['input_ids', 'attention_mask', 'labels'])\n",
      "Lengths: 3 1\n",
      "First 20 input_ids: [3, 3, 2]\n",
      "First 20 label_ids: [2]\n",
      "\n",
      "Decoded Sanskrit (keep special tokens):\n",
      "<unk><unk></s>\n",
      "\n",
      "Decoded English (keep special tokens):\n",
      "</s>\n"
     ]
    }
   ],
   "source": [
    "example = tokenized_datasets[\"train\"][0]\n",
    "input_ids = example[\"input_ids\"]\n",
    "label_ids = example[\"labels\"]\n",
    "\n",
    "print(\"Keys:\", example.keys())\n",
    "print(\"Lengths:\", len(input_ids), len(label_ids))\n",
    "print(\"First 20 input_ids:\", input_ids[:20])\n",
    "print(\"First 20 label_ids:\", label_ids[:20])\n",
    "\n",
    "print(\"\\nDecoded Sanskrit (keep special tokens):\")\n",
    "print(tokenizer.decode(input_ids, skip_special_tokens=False))\n",
    "\n",
    "print(\"\\nDecoded English (keep special tokens):\")\n",
    "print(tokenizer.decode(label_ids, skip_special_tokens=False))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "8eefae6f-4306-4aa9-bdf7-6e5893ca4ed8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ab819eae730c4ef0ae6a0a54d41e2afe",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/67644 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c07931346c854c98832716ac24456789",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/7517 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "SRC_LANG_TAG = \"<2sa>\"\n",
    "TGT_LANG_TAG = \"<2en>\"\n",
    "\n",
    "max_source_length = 256\n",
    "max_target_length = 256\n",
    "\n",
    "def preprocess_function(examples):\n",
    "    inputs = [f\"{SRC_LANG_TAG} {TGT_LANG_TAG} {text}\" for text in examples[\"src_sa\"]]\n",
    "    targets = [f\"{TGT_LANG_TAG} {text}\" for text in examples[\"tgt_en\"]]\n",
    "\n",
    "    model_inputs = tokenizer(\n",
    "        inputs,\n",
    "        max_length=max_source_length,\n",
    "        truncation=True,\n",
    "    )\n",
    "\n",
    "    with tokenizer.as_target_tokenizer():\n",
    "        labels = tokenizer(\n",
    "            targets,\n",
    "            max_length=max_target_length,\n",
    "            truncation=True,\n",
    "        )\n",
    "\n",
    "    model_inputs[\"labels\"] = labels[\"input_ids\"]\n",
    "    return model_inputs\n",
    "\n",
    "tokenized_datasets = raw_datasets.map(\n",
    "    preprocess_function,\n",
    "    batched=True,\n",
    "    remove_columns=raw_datasets[\"train\"].column_names,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "46485a3e-43a6-48c9-bb7d-0b42c1c709f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import TrainingArguments\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=str(LOG_DIR / \"sa_en_indictrans2_ft\"),\n",
    "    overwrite_output_dir=True,\n",
    "\n",
    "    # new name in recent transformers versions\n",
    "    eval_strategy=\"epoch\",          # instead of evaluation_strategy=\"epoch\"\n",
    "\n",
    "    save_strategy=\"epoch\",\n",
    "    logging_strategy=\"steps\",\n",
    "    logging_steps=50,\n",
    "\n",
    "    num_train_epochs=1,\n",
    "    per_device_train_batch_size=2,\n",
    "    per_device_eval_batch_size=2,\n",
    "    gradient_accumulation_steps=4,\n",
    "    learning_rate=2e-5,\n",
    "    weight_decay=0.01,\n",
    "    fp16=False,\n",
    "    report_to=\"none\",\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "01bc701f-2583-40ef-ab67-31052f47539e",
   "metadata": {},
   "outputs": [],
   "source": [
    "training_args = TrainingArguments(\n",
    "    output_dir=str(LOG_DIR / \"sa_en_indictrans2_ft\"),\n",
    "    overwrite_output_dir=True,\n",
    "    num_train_epochs=1,\n",
    "    per_device_train_batch_size=2,\n",
    "    per_device_eval_batch_size=2,\n",
    "    gradient_accumulation_steps=4,\n",
    "    learning_rate=2e-5,\n",
    "    weight_decay=0.01,\n",
    "    fp16=False,\n",
    "    report_to=\"none\",\n",
    ")\n",
    "\n",
    "training_args = training_args.set_evaluate(strategy=\"epoch\")\n",
    "training_args = training_args.set_save(strategy=\"epoch\")\n",
    "training_args = training_args.set_logging(strategy=\"steps\", steps=50, report_to=\"none\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "fc4cbb35-919b-417d-952f-d76c47efcc5f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/p7/h3z23h2127x2jrspfn8fy4h40000gn/T/ipykernel_39080/2138141152.py:1: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n",
      "  trainer = Trainer(\n"
     ]
    }
   ],
   "source": [
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=tokenized_datasets[\"train\"],\n",
    "    eval_dataset=tokenized_datasets[\"validation\"],\n",
    "    tokenizer=tokenizer,\n",
    "    data_collator=data_collator,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "283566f6-2421-46cd-a3a2-006a79ec8ca5",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (Ancient-AI)",
   "language": "python",
   "name": "ancient-ai-env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
