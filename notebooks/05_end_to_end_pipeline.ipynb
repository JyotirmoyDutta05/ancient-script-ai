{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "b47a67ac-1a3a-4c18-a07e-2493de333e8d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pipeline initialized âœ…\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import logging\n",
    "from pathlib import Path\n",
    "from PIL import Image\n",
    "import torch\n",
    "from transformers import pipeline\n",
    "import joblib\n",
    "import numpy as np\n",
    "\n",
    "from torchvision import transforms\n",
    "from torch import nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "# Disable TF and set up logging\n",
    "os.environ[\"USE_TF\"] = \"0\"\n",
    "logging.basicConfig(level=logging.INFO, format=\"[%(levelname)s] %(message)s\")\n",
    "\n",
    "PROJECT_ROOT = Path.cwd().resolve().parents[0] if Path.cwd().name == \"notebooks\" else Path.cwd()\n",
    "DATA_ROOT = PROJECT_ROOT / \"data\"\n",
    "MODELS_ROOT = PROJECT_ROOT / \"models\"\n",
    "print(\"Pipeline initialized âœ…\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "d0a59b24-dd7a-4eb3-8fad-d17427804554",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[INFO] OCR model loaded âœ…  (devanagari_cnn_v1.pth)\n",
      "[INFO] Loaded 46 character classes.\n"
     ]
    }
   ],
   "source": [
    "class OCRModel(nn.Module):\n",
    "    def __init__(self, num_classes=46):\n",
    "        super(OCRModel, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(1, 32, 3, 1)\n",
    "        self.conv2 = nn.Conv2d(32, 64, 3, 1)\n",
    "        self.conv3 = nn.Conv2d(64, 128, 3, 1)\n",
    "        self.fc1 = nn.Linear(128 * 8 * 8, 256)\n",
    "        self.fc2 = nn.Linear(256, num_classes)\n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.conv1(x))\n",
    "        x = F.relu(self.conv2(x))\n",
    "        x = F.relu(self.conv3(x))\n",
    "        x = F.max_pool2d(x, 2)\n",
    "        x = torch.flatten(x, 1)\n",
    "        x = F.relu(self.fc1(x))\n",
    "        return self.fc2(x)\n",
    "\n",
    "ocr_model_path = MODELS_ROOT / \"devanagari_cnn_v1.pth\"\n",
    "ocr_model = OCRModel(num_classes=46)  # adjust num_classes if your dataset differs\n",
    "\n",
    "# Safe model loading\n",
    "if ocr_model_path.exists():\n",
    "    checkpoint = torch.load(ocr_model_path, map_location=\"cpu\")\n",
    "    ocr_model.load_state_dict(checkpoint[\"model_state_dict\"])\n",
    "    class_names = checkpoint.get(\"class_names\", [])\n",
    "    ocr_model.eval()\n",
    "    logging.info(f\"OCR model loaded âœ…  ({ocr_model_path.name})\")\n",
    "    if class_names:\n",
    "        logging.info(f\"Loaded {len(class_names)} character classes.\")\n",
    "else:\n",
    "    logging.warning(f\"OCR model not found at {ocr_model_path}, using untrained model ðŸš§\")\n",
    "    class_names = []\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "57b6807e-9400-4964-8c79-d82bf8263734",
   "metadata": {},
   "outputs": [],
   "source": [
    "from PIL import Image\n",
    "from torchvision import transforms\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "def preprocess_image(image_path):\n",
    "    \"\"\"\n",
    "    Load an image, convert to grayscale, resize to 64x64 (same as training),\n",
    "    normalize, and add batch dimension.\n",
    "    \"\"\"\n",
    "    image = Image.open(image_path).convert(\"L\")  # grayscale\n",
    "\n",
    "    transform = transforms.Compose([\n",
    "        transforms.Resize((64, 64)),              # MUST match training size\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize(mean=[0.5], std=[0.5]),\n",
    "    ])\n",
    "\n",
    "    tensor = transform(image).unsqueeze(0)       # shape: [1, 1, 64, 64]\n",
    "    # Optional debug print:\n",
    "    # print(\"DEBUG tensor shape:\", tensor.shape)\n",
    "\n",
    "    return tensor.to(device)\n",
    "\n",
    "\n",
    "def ocr_predict(image_tensor):\n",
    "    \"\"\"Predict character from OCR model.\"\"\"\n",
    "    with torch.no_grad():\n",
    "        logits = ocr_model(image_tensor)\n",
    "        pred = torch.argmax(logits, dim=1).item()\n",
    "    return pred\n",
    "\n",
    "def transliterate_character(pred_index, mapping_dict):\n",
    "    \"\"\"Convert model output index â†’ phonetic representation.\"\"\"\n",
    "    return mapping_dict.get(pred_index, \"?\")\n",
    "\n",
    "def reconstruct_text(sequence, bigram_probs):\n",
    "    \"\"\"Use probabilistic smoothing to predict missing pieces.\"\"\"\n",
    "    text = \"\".join(sequence)\n",
    "    for word, prob in bigram_probs.items():\n",
    "        if word in text:\n",
    "            text = text.replace(word, word)  # placeholder for actual logic\n",
    "    return text\n",
    "\n",
    "def detect_language(text):\n",
    "    X = vectorizer.transform([text])\n",
    "    probs = lang_model.predict_proba(X)[0]\n",
    "    classes = lang_model.classes_\n",
    "    return classes[np.argmax(probs)], np.max(probs)\n",
    "\n",
    "def translate_text(text, lang):\n",
    "    result = translator(text, clean_up_tokenization_spaces=True)\n",
    "    return result[0][\"translation_text\"]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "1a30ec27-c493-4a34-a0d9-595411d1b572",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[INFO] Loaded 112 transliteration mappings âœ…\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Columns: ['Unicode', 'Relative Offset', 'Devanagari', 'ITRANS', 'Notes', 'Valid Vector Representation', 'is_vowel', 'is_consonant', 'nukta', 'halanta', 'anusvara', 'misc', 'short_vowel', 'long_vowel', 'weak', 'medium', 'strong', 'independent_vowel', 'dependent_vowel', 'plosive', 'fricative', 'Central-approximant', 'Lateral-approximant', 'flap', 'velar', 'palatal', 'retroflex', 'dental', 'labial', 'aspirated', 'not_aspirated', 'voiced', 'unvoiced', 'nasal', 'not_nasal', 'front', 'central', 'back', 'close', 'close-mid', 'open-mid', 'open', 'rounded', 'not_rounded']\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "mapping_file = DATA_ROOT / \"phonetic_mappings\" / \"all_script_phonetic_data.csv\"\n",
    "\n",
    "# Load the CSV\n",
    "mapping_df = pd.read_csv(mapping_file)\n",
    "\n",
    "# Confirm structure\n",
    "print(\"Columns:\", mapping_df.columns.tolist())\n",
    "\n",
    "# Build dictionary safely using attribute access\n",
    "if \"ITRANS\" in mapping_df.columns:\n",
    "    mapping_dict = {i: getattr(row, \"ITRANS\") for i, row in enumerate(mapping_df.itertuples())}\n",
    "else:\n",
    "    # If the column name differs slightly, like \"itrans\" or \"Itrans\", normalize\n",
    "    itrans_col = [c for c in mapping_df.columns if c.lower() == \"itrans\"][0]\n",
    "    mapping_dict = {i: getattr(row, itrans_col) for i, row in enumerate(mapping_df.itertuples())}\n",
    "\n",
    "logging.info(f\"Loaded {len(mapping_dict)} transliteration mappings âœ…\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "e319a12c-ae14-4b36-80c4-a218e9618f94",
   "metadata": {},
   "outputs": [],
   "source": [
    "class AncientScriptAI:\n",
    "    def __init__(self):\n",
    "        self.ocr_model = ocr_model\n",
    "        self.mapping = mapping_dict\n",
    "        self.lang_clf = lang_model\n",
    "        self.vectorizer = vectorizer\n",
    "        self.translator = translator\n",
    "\n",
    "    def process_image(self, image_path):\n",
    "        logging.info(f\"Processing {image_path.name}\")\n",
    "        tensor = preprocess_image(image_path)\n",
    "        pred_idx = ocr_predict(tensor)\n",
    "        translit = transliterate_character(pred_idx, self.mapping)\n",
    "        reconstructed = reconstruct_text([translit], {})\n",
    "        lang, conf = detect_language(reconstructed)\n",
    "        translation = translate_text(reconstructed, lang)\n",
    "        return {\n",
    "            \"image\": str(image_path),\n",
    "            \"recognized_text\": translit,\n",
    "            \"reconstructed_text\": reconstructed,\n",
    "            \"language\": lang,\n",
    "            \"confidence\": round(conf, 3),\n",
    "            \"translation\": translation\n",
    "        }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "2b8c55a5-51fc-4ed8-bd5a-ed86ac61ed0b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Models directory set to: /Users/jyotirmoy/Desktop/Image/ancient-script-ai/models\n"
     ]
    }
   ],
   "source": [
    "from pathlib import Path\n",
    "\n",
    "# Set up your base directories\n",
    "PROJECT_ROOT = Path.cwd().resolve().parents[0] if Path.cwd().name == \"notebooks\" else Path.cwd()\n",
    "MODELS_ROOT = PROJECT_ROOT / \"models\"\n",
    "MODELS_ROOT.mkdir(exist_ok=True)\n",
    "\n",
    "print(\"Models directory set to:\", MODELS_ROOT)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "6cf5ae30-37bc-428c-89c7-f607ead1982b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Language model retrained and saved successfully!\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "import joblib\n",
    "import pandas as pd\n",
    "\n",
    "# Small starter dataset (you can expand it)\n",
    "data = {\n",
    "    \"text\": [\n",
    "        \"à¤•à¤°à¥à¤®à¤£à¤¾ à¤œà¤¾à¤¯à¤¤à¥‡ à¤ªà¥à¤°à¥à¤·à¤ƒ\", \"à¤œà¥à¤žà¤¾à¤¨à¤‚ à¤¯à¥‹à¤—à¥‡à¤¨ à¤¸à¤¾à¤§à¥à¤¯à¤¤à¥‡\",   # Sanskrit\n",
    "        \"à¤®à¥‡à¤°à¤¾ à¤˜à¤° à¤¬à¤¡à¤¼à¤¾ à¤¹à¥ˆ\", \"à¤µà¤¹ à¤¬à¤¾à¤œà¤¼à¤¾à¤° à¤—à¤¯à¤¾ à¤¥à¤¾\",           # Hindi\n",
    "        \"à¤®à¤¾à¤à¥‡ à¤˜à¤° à¤®à¥‹à¤ à¥‡ à¤†à¤¹à¥‡\", \"à¤¤à¥‹ à¤¬à¤¾à¤œà¤¾à¤°à¤¾à¤¤ à¤—à¥‡à¤²à¤¾ à¤¹à¥‹à¤¤à¤¾\"        # Marathi\n",
    "    ],\n",
    "    \"language\": [\"sanskrit\", \"sanskrit\", \"hindi\", \"hindi\", \"marathi\", \"marathi\"]\n",
    "}\n",
    "\n",
    "lang_df = pd.DataFrame(data)\n",
    "\n",
    "# Train Naive Bayes classifier\n",
    "vectorizer = CountVectorizer(analyzer=\"char\", ngram_range=(1,3))\n",
    "X = vectorizer.fit_transform(lang_df[\"text\"])\n",
    "y = lang_df[\"language\"]\n",
    "\n",
    "lang_model = MultinomialNB()\n",
    "lang_model.fit(X, y)\n",
    "\n",
    "# Save models freshly compiled in this environment\n",
    "MODELS_ROOT.mkdir(exist_ok=True)\n",
    "joblib.dump(lang_model, MODELS_ROOT / \"language_id_model.pkl\")\n",
    "joblib.dump(vectorizer, MODELS_ROOT / \"language_vectorizer.pkl\")\n",
    "\n",
    "print(\"âœ… Language model retrained and saved successfully!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "d3e27b3e-ee4e-4aeb-a634-822e148a8cd2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use mps:0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Translation pipeline loaded âœ…\n"
     ]
    }
   ],
   "source": [
    "from transformers import pipeline\n",
    "import os\n",
    "\n",
    "# Ensure TensorFlow stays disabled\n",
    "os.environ[\"USE_TF\"] = \"0\"\n",
    "\n",
    "# Load the multilingual MarianMT model for translation\n",
    "translator = pipeline(\n",
    "    \"translation\",\n",
    "    model=\"Helsinki-NLP/opus-mt-mul-en\",\n",
    "    framework=\"pt\"\n",
    ")\n",
    "\n",
    "print(\"Translation pipeline loaded âœ…\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "57c8c597-b7dd-4458-98b2-6a3faaa80184",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[INFO] Processing test_devanagari_char.png\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Language ID model and vectorizer loaded âœ…\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "mat1 and mat2 shapes cannot be multiplied (1x107648 and 8192x256)",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mRuntimeError\u001b[39m                              Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[28]\u001b[39m\u001b[32m, line 16\u001b[39m\n\u001b[32m     13\u001b[39m pipeline_ai = AncientScriptAI()\n\u001b[32m     15\u001b[39m sample_image = DATA_ROOT / \u001b[33m\"\u001b[39m\u001b[33msamples\u001b[39m\u001b[33m\"\u001b[39m / \u001b[33m\"\u001b[39m\u001b[33mtest_devanagari_char.png\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m---> \u001b[39m\u001b[32m16\u001b[39m result = \u001b[43mpipeline_ai\u001b[49m\u001b[43m.\u001b[49m\u001b[43mprocess_image\u001b[49m\u001b[43m(\u001b[49m\u001b[43msample_image\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     18\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33mðŸ“œ Final Result:\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m     19\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m k, v \u001b[38;5;129;01min\u001b[39;00m result.items():\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[24]\u001b[39m\u001b[32m, line 12\u001b[39m, in \u001b[36mAncientScriptAI.process_image\u001b[39m\u001b[34m(self, image_path)\u001b[39m\n\u001b[32m     10\u001b[39m logging.info(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mProcessing \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mimage_path.name\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n\u001b[32m     11\u001b[39m tensor = preprocess_image(image_path)\n\u001b[32m---> \u001b[39m\u001b[32m12\u001b[39m pred_idx = \u001b[43mocr_predict\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtensor\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     13\u001b[39m translit = transliterate_character(pred_idx, \u001b[38;5;28mself\u001b[39m.mapping)\n\u001b[32m     14\u001b[39m reconstructed = reconstruct_text([translit], {})\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[22]\u001b[39m\u001b[32m, line 29\u001b[39m, in \u001b[36mocr_predict\u001b[39m\u001b[34m(image_tensor)\u001b[39m\n\u001b[32m     27\u001b[39m \u001b[38;5;250m\u001b[39m\u001b[33;03m\"\"\"Predict character from OCR model.\"\"\"\u001b[39;00m\n\u001b[32m     28\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m torch.no_grad():\n\u001b[32m---> \u001b[39m\u001b[32m29\u001b[39m     logits = \u001b[43mocr_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimage_tensor\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     30\u001b[39m     pred = torch.argmax(logits, dim=\u001b[32m1\u001b[39m).item()\n\u001b[32m     31\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m pred\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/Image/ancient-script-ai/ancient-ai-env/lib/python3.13/site-packages/torch/nn/modules/module.py:1775\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1773\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1774\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1775\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/Image/ancient-script-ai/ancient-ai-env/lib/python3.13/site-packages/torch/nn/modules/module.py:1786\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1781\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1782\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1783\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1784\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1785\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1786\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1788\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1789\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[21]\u001b[39m\u001b[32m, line 15\u001b[39m, in \u001b[36mOCRModel.forward\u001b[39m\u001b[34m(self, x)\u001b[39m\n\u001b[32m     13\u001b[39m x = F.max_pool2d(x, \u001b[32m2\u001b[39m)\n\u001b[32m     14\u001b[39m x = torch.flatten(x, \u001b[32m1\u001b[39m)\n\u001b[32m---> \u001b[39m\u001b[32m15\u001b[39m x = F.relu(\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mfc1\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[32m     16\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m.fc2(x)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/Image/ancient-script-ai/ancient-ai-env/lib/python3.13/site-packages/torch/nn/modules/module.py:1775\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1773\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1774\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1775\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/Image/ancient-script-ai/ancient-ai-env/lib/python3.13/site-packages/torch/nn/modules/module.py:1786\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1781\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1782\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1783\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1784\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1785\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1786\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1788\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1789\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/Image/ancient-script-ai/ancient-ai-env/lib/python3.13/site-packages/torch/nn/modules/linear.py:134\u001b[39m, in \u001b[36mLinear.forward\u001b[39m\u001b[34m(self, input)\u001b[39m\n\u001b[32m    130\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor) -> Tensor:\n\u001b[32m    131\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m    132\u001b[39m \u001b[33;03m    Runs the forward pass.\u001b[39;00m\n\u001b[32m    133\u001b[39m \u001b[33;03m    \"\"\"\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m134\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[43m.\u001b[49m\u001b[43mlinear\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mbias\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[31mRuntimeError\u001b[39m: mat1 and mat2 shapes cannot be multiplied (1x107648 and 8192x256)"
     ]
    }
   ],
   "source": [
    "\n",
    "import joblib\n",
    "\n",
    "# Paths to your trained models\n",
    "lang_model_path = MODELS_ROOT / \"language_id_model.pkl\"\n",
    "vectorizer_path = MODELS_ROOT / \"language_vectorizer.pkl\"\n",
    "\n",
    "# Load models\n",
    "lang_model = joblib.load(lang_model_path)\n",
    "vectorizer = joblib.load(vectorizer_path)\n",
    "\n",
    "print(\"Language ID model and vectorizer loaded âœ…\")\n",
    "\n",
    "pipeline_ai = AncientScriptAI()\n",
    "\n",
    "sample_image = DATA_ROOT / \"samples\" / \"test_devanagari_char.png\"\n",
    "result = pipeline_ai.process_image(sample_image)\n",
    "\n",
    "print(\"\\nðŸ“œ Final Result:\")\n",
    "for k, v in result.items():\n",
    "    print(f\"{k:20}: {v}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "e14c0262-6769-464a-988b-bfdbdecf02c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class ScriptCNN(nn.Module):\n",
    "    \"\"\"\n",
    "    CNN architecture MUST match the one used during training.\n",
    "    64x64 â†’ conv/pool â†’ 32x32 â†’ 16x16 â†’ 8x8 â†’ flatten 8192.\n",
    "    \"\"\"\n",
    "    def __init__(self, num_classes):\n",
    "        super().__init__()\n",
    "        self.conv1 = nn.Conv2d(1, 32, kernel_size=3, padding=1)\n",
    "        self.conv2 = nn.Conv2d(32, 64, kernel_size=3, padding=1)\n",
    "        self.conv3 = nn.Conv2d(64, 128, kernel_size=3, padding=1)\n",
    "\n",
    "        self.pool = nn.MaxPool2d(2, 2)  # halves H and W\n",
    "\n",
    "        # 64x64 â†’ pool â†’ 32x32 â†’ pool â†’ 16x16 â†’ pool â†’ 8x8\n",
    "        self.fc1 = nn.Linear(128 * 8 * 8, 256)  # 8192 inputs\n",
    "        self.fc2 = nn.Linear(256, num_classes)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.pool(F.relu(self.conv1(x)))\n",
    "        x = self.pool(F.relu(self.conv2(x)))\n",
    "        x = self.pool(F.relu(self.conv3(x)))\n",
    "        x = x.view(x.size(0), -1)  # flatten: should be (batch, 8192)\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = self.fc2(x)\n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "a88e4c66-4048-41ba-ac2a-38a12729c268",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded OCR model âœ…\n",
      "Num classes: 46\n"
     ]
    }
   ],
   "source": [
    "from pathlib import Path\n",
    "\n",
    "MODELS_ROOT = PROJECT_ROOT / \"models\"\n",
    "ckpt_path = MODELS_ROOT / \"devanagari_cnn_v1.pth\"  # <-- change if your file name differs\n",
    "\n",
    "checkpoint = torch.load(ckpt_path, map_location=\"cpu\")\n",
    "class_names = checkpoint[\"class_names\"]\n",
    "\n",
    "ocr_model = ScriptCNN(num_classes=len(class_names))\n",
    "ocr_model.load_state_dict(checkpoint[\"model_state_dict\"])\n",
    "ocr_model.eval()\n",
    "\n",
    "print(\"Loaded OCR model âœ…\")\n",
    "print(\"Num classes:\", len(class_names))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "f0481492-bedd-4eee-a4f7-0872a97e1ea0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchvision import transforms\n",
    "from PIL import Image\n",
    "\n",
    "IMG_SIZE = 64\n",
    "\n",
    "inference_transform = transforms.Compose([\n",
    "    transforms.Resize((IMG_SIZE, IMG_SIZE)),\n",
    "    transforms.ToTensor(),      # NO normalization if you didn't use it in training\n",
    "])\n",
    "\n",
    "\n",
    "device = torch.device(\"mps\" if torch.mps.is_available() else \"cpu\")\n",
    "\n",
    "def preprocess_image(image_path):\n",
    "    \"\"\"\n",
    "    Load an image, convert to grayscale, resize to 64x64 (same as training),\n",
    "    normalize, and add batch dimension.\n",
    "    \"\"\"\n",
    "    image = Image.open(image_path).convert(\"L\")  # grayscale\n",
    "\n",
    "    transform = transforms.Compose([\n",
    "        transforms.Resize((64, 64)),              # MUST match training size\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize(mean=[0.5], std=[0.5]),\n",
    "    ])\n",
    "\n",
    "    tensor = transform(image).unsqueeze(0)       # shape: [1, 1, 64, 64]\n",
    "    # Optional debug print:\n",
    "    # print(\"DEBUG tensor shape:\", tensor.shape)\n",
    "\n",
    "    return tensor.to(device)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "1e69c0ee-9d7d-44ef-842d-a4edc7e7afb2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 1, 64, 64])\n"
     ]
    }
   ],
   "source": [
    "t = preprocess_image(sample_image)\n",
    "print(t.shape)   # should be: torch.Size([1, 1, 64, 64])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "72ca00cc-fd5f-4db6-890c-2604daf0f6a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def ocr_predict(image_tensor: torch.Tensor) -> int:\n",
    "    \"\"\"Return predicted class index for a single image tensor.\"\"\"\n",
    "    with torch.no_grad():\n",
    "        logits = ocr_model(image_tensor)\n",
    "        pred_idx = torch.argmax(logits, dim=1).item()\n",
    "    return pred_idx\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "ea84116e-56d0-45a8-af9e-d048ded431a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def ocr_predict_label(image_tensor: torch.Tensor) -> str:\n",
    "    idx = ocr_predict(image_tensor)\n",
    "    return class_names[idx]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "bdddd876-e135-4a05-afe8-18589cc53154",
   "metadata": {},
   "outputs": [],
   "source": [
    "class AncientScriptAI:\n",
    "    def __init__(self):\n",
    "        self.ocr_model = ocr_model\n",
    "        self.class_names = class_names\n",
    "        self.mapping = mapping_dict              # from your transliteration CSV\n",
    "        self.lang_clf = lang_model              # joblib loaded\n",
    "        self.vectorizer = vectorizer\n",
    "        self.translator = translator\n",
    "\n",
    "    def process_image(self, image_path):\n",
    "        logging.info(f\"Processing {image_path.name}\")\n",
    "        \n",
    "        tensor = preprocess_image(image_path)\n",
    "        pred_idx = ocr_predict(tensor)\n",
    "        class_label = self.class_names[pred_idx]\n",
    "\n",
    "        # OPTIONAL: map class_label â†’ actual Devanagari char if you have such a dict\n",
    "        # For now weâ€™ll assume transliteration mapping is by character index:\n",
    "        translit = transliterate_character(pred_idx, self.mapping)\n",
    "\n",
    "        reconstructed = reconstruct_text([translit], {})  # simple for now\n",
    "        lang, conf = detect_language(reconstructed)\n",
    "        translation = translate_text(reconstructed, lang)\n",
    "\n",
    "        return {\n",
    "            \"image\": str(image_path),\n",
    "            \"predicted_class\": class_label,\n",
    "            \"recognized_text\": translit,\n",
    "            \"reconstructed_text\": reconstructed,\n",
    "            \"language\": lang,\n",
    "            \"confidence\": round(conf, 3),\n",
    "            \"translation\": translation\n",
    "        }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "151725ca-72a0-4740-be41-1a82a162855e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of classes: 46\n",
      "0 character_10_yna\n",
      "1 character_11_taa\n",
      "2 character_12_thaa\n",
      "3 character_13_daa\n",
      "4 character_14_dhaa\n",
      "5 character_15_adna\n",
      "6 character_16_tabala\n",
      "7 character_17_tha\n",
      "8 character_18_da\n",
      "9 character_19_dha\n",
      "10 character_1_ka\n",
      "11 character_20_na\n",
      "12 character_21_pa\n",
      "13 character_22_pha\n",
      "14 character_23_ba\n",
      "15 character_24_bha\n",
      "16 character_25_ma\n",
      "17 character_26_yaw\n",
      "18 character_27_ra\n",
      "19 character_28_la\n",
      "20 character_29_waw\n",
      "21 character_2_kha\n",
      "22 character_30_motosaw\n",
      "23 character_31_petchiryakha\n",
      "24 character_32_patalosaw\n",
      "25 character_33_ha\n",
      "26 character_34_chhya\n",
      "27 character_35_tra\n",
      "28 character_36_gya\n",
      "29 character_3_ga\n",
      "30 character_4_gha\n",
      "31 character_5_kna\n",
      "32 character_6_cha\n",
      "33 character_7_chha\n",
      "34 character_8_ja\n",
      "35 character_9_jha\n",
      "36 digit_0\n",
      "37 digit_1\n",
      "38 digit_2\n",
      "39 digit_3\n",
      "40 digit_4\n",
      "41 digit_5\n",
      "42 digit_6\n",
      "43 digit_7\n",
      "44 digit_8\n",
      "45 digit_9\n"
     ]
    }
   ],
   "source": [
    "# Assuming you already loaded your checkpoint\n",
    "# and have `class_names` like in 01 notebook:\n",
    "\n",
    "print(\"Number of classes:\", len(class_names))\n",
    "for i, name in enumerate(class_names):\n",
    "    print(i, name)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "9df430f6-1590-452f-b7da-67326b1965a0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Still to fill: ['character_11_taa']\n"
     ]
    }
   ],
   "source": [
    "\n",
    "class_to_dev = {\n",
    "    \"character_1_ka\":  \"à¤•\",\n",
    "    \"character_2_kha\": \"à¤–\",\n",
    "    \"character_3_ga\":  \"à¤—\",\n",
    "    \"character_4_gha\": \"à¤˜\",\n",
    "    \"character_5_kna\": \"à¤™\",\n",
    "    \"character_6_cha\":  \"à¤š\",\n",
    "    \"character_7_chha\": \"à¤›\",\n",
    "    \"character_8_ja\":  \"à¤œ\",\n",
    "    \"character_9_jha\": \"à¤\",\n",
    "    \"character_10_yna\": \"à¤ž\",\n",
    "    \"character_11_taamatar\": \"à¤Ÿ\",\n",
    "    \"character_12_thaa\": \"à¤ \",\n",
    "    \"character_13_daa\": \"à¤¡\",\n",
    "    \"character_14_dhaa\": \"à¤¢\",\n",
    "    \"character_15_adna\": \"à¤£\",\n",
    "    \"character_16_tabala\": \"à¤¤\",\n",
    "    \"character_17_tha\": \"à¤¥\",\n",
    "    \"character_18_da\": \"à¤¦\",\n",
    "    \"character_19_dha\": \"à¤§\",\n",
    "    \"character_20_na\": \"à¤¨\",\n",
    "    \"character_21_pa\": \"à¤ª\",\n",
    "    \"character_22_pha\": \"à¤«\",\n",
    "    \"character_23_ba\": \"à¤¬\",\n",
    "    \"character_24_bha\": \"à¤­\",\n",
    "    \"character_25_ma\": \"à¤®\",\n",
    "    \"character_26_yaw\": \"à¤¯\",\n",
    "    \"character_27_ra\": \"à¤°\",\n",
    "    \"character_28_la\": \"à¤²\",\n",
    "    \"character_29_waw\": \"à¤µ\",\n",
    "    \"character_30_motosaw\": \"à¤¶\",\n",
    "    \"character_31_petchiryakha\": \"à¤·\",\n",
    "    \"character_32_patalosaw\": \"à¤¸\",\n",
    "    \"character_33_ha\": \"à¤¹\",\n",
    "    \"character_34_chhya\": \"à¤•à¥à¤·\",\n",
    "    \"character_35_tra\": \"à¤¤à¥à¤°\",\n",
    "    \"character_36_gya\": \"à¤œà¥à¤ž\",\n",
    "    \"digit_0\": \"à¥¦\",\n",
    "    \"digit_1\": \"à¥§\", \n",
    "    \"digit_2\": \"à¥¨\",\n",
    "    \"digit_3\": \"à¥©\",\n",
    "    \"digit_4\": \"à¥ª\",\n",
    "    \"digit_5\": \"à¥«\",\n",
    "    \"digit_6\": \"à¥¬\",\n",
    "    \"digit_7\": \"à¥­\",\n",
    "    \"digit_8\": \"à¥®\",\n",
    "    \"digit_9\": \"à¥¯\"\n",
    "\n",
    "    \n",
    "}\n",
    "\n",
    "missing = [c for c in class_names if c not in class_to_dev]\n",
    "print(\"Still to fill:\", missing)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "ff2e1624-e7de-447d-baf1-400d37796d7f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total mapping entries: 111\n",
      "Example: [('à¤€', 'à¤€'), ('à¤', '.n'), ('à¤‚', '.n'), ('à¤ƒ', 'H'), ('à¤„', 'à¤„'), ('à¤…', 'a'), ('à¤†', 'A'), ('à¤‡', 'i'), ('à¤ˆ', 'I'), ('à¤‰', 'u')]\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "\n",
    "phonetic_path = DATA_ROOT / \"phonetic_mappings\"/ \"all_script_phonetic_data.csv\"\n",
    "phonetic_df = pd.read_csv(phonetic_path)\n",
    "\n",
    "# keep only rows that have both Devanagari and ITRANS\n",
    "phonetic_df = phonetic_df.dropna(subset=[\"Devanagari\", \"ITRANS\"])\n",
    "\n",
    "dev_to_itrans = dict(zip(phonetic_df[\"Devanagari\"], phonetic_df[\"ITRANS\"]))\n",
    "\n",
    "print(\"Total mapping entries:\", len(dev_to_itrans))\n",
    "print(\"Example:\", list(dev_to_itrans.items())[:10])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "6cdb2365-cd79-493a-a1d5-113c052357e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def class_to_translit(class_label: str):\n",
    "    \"\"\"\n",
    "    Convert a CNN class label like 'character_1_ka'\n",
    "    into (devanagari_char, itrans_string).\n",
    "    \"\"\"\n",
    "    dev_char = class_to_dev.get(class_label, \"?\")\n",
    "    itrans = dev_to_itrans.get(dev_char, dev_char)\n",
    "    return dev_char, itrans\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "39aeb3c0-e12c-4c28-a31b-ed2573ad0f28",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('à¤•', 'ka')\n",
      "('à¤¬', 'ba')\n"
     ]
    }
   ],
   "source": [
    "print(class_to_translit(\"character_1_ka\"))   # should be ('à¤•', 'ka')\n",
    "print(class_to_translit(\"character_23_ba\"))  # should be ('à¤¬', 'ba') after you fill mapping\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "8c7662be-ca1e-48f6-b2ad-3bde0b3f9202",
   "metadata": {},
   "outputs": [],
   "source": [
    "class AncientScriptAI:\n",
    "    def __init__(self):\n",
    "        self.ocr_model = ocr_model\n",
    "        self.class_names = class_names\n",
    "        self.class_to_dev = class_to_dev\n",
    "        self.dev_to_itrans = dev_to_itrans\n",
    "\n",
    "        # you still have lang_clf, vectorizer, translator\n",
    "        # but we won't use them for single chars\n",
    "\n",
    "    def process_image(self, image_path):\n",
    "        logging.info(f\"Processing {image_path.name}\")\n",
    "\n",
    "        tensor = preprocess_image(image_path)  # 64x64 â†’ tensor\n",
    "        pred_idx = ocr_predict(tensor)\n",
    "        class_label = self.class_names[pred_idx]\n",
    "\n",
    "        dev_char, itrans = class_to_translit(class_label)\n",
    "\n",
    "        # For a single character, STOP here.\n",
    "        # Don't run language ID or translation yet.\n",
    "        return {\n",
    "            \"image\": str(image_path),\n",
    "            \"predicted_class\": class_label,\n",
    "            \"recognized_devanagari\": dev_char,\n",
    "            \"recognized_itrans\": itrans,\n",
    "        }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "cc5f5ec2-b3d2-4bf4-950f-fb8519e41f6d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[INFO] Processing test_devanagari_char.png\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ðŸ“œ Final Result:\n",
      "image               : /Users/jyotirmoy/Desktop/Image/ancient-script-ai/data/samples/test_devanagari_char.png\n",
      "predicted_class     : character_1_ka\n",
      "recognized_devanagari: à¤•\n",
      "recognized_itrans   : ka\n"
     ]
    }
   ],
   "source": [
    "pipeline_ai = AncientScriptAI()\n",
    "\n",
    "sample_image = DATA_ROOT / \"samples\" / \"test_devanagari_char.png\"\n",
    "result = pipeline_ai.process_image(sample_image)\n",
    "\n",
    "print(\"\\nðŸ“œ Final Result:\")\n",
    "for k, v in result.items():\n",
    "    print(f\"{k:20}: {v}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "e3f8cb46-b69a-4fe2-800b-9ed4f3cadced",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'result' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[42]\u001b[39m\u001b[32m, line 8\u001b[39m\n\u001b[32m      5\u001b[39m output_file = outputs_dir / \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mresult_\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtimestamp\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m.json\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m      7\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mopen\u001b[39m(output_file, \u001b[33m\"\u001b[39m\u001b[33mw\u001b[39m\u001b[33m\"\u001b[39m, encoding=\u001b[33m\"\u001b[39m\u001b[33mutf-8\u001b[39m\u001b[33m\"\u001b[39m) \u001b[38;5;28;01mas\u001b[39;00m f:\n\u001b[32m----> \u001b[39m\u001b[32m8\u001b[39m     json.dump(\u001b[43mresult\u001b[49m, f, ensure_ascii=\u001b[38;5;28;01mFalse\u001b[39;00m, indent=\u001b[32m4\u001b[39m)\n\u001b[32m     10\u001b[39m logging.info(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mSaved output to \u001b[39m\u001b[38;5;132;01m{\u001b[39;00moutput_file\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n",
      "\u001b[31mNameError\u001b[39m: name 'result' is not defined"
     ]
    }
   ],
   "source": [
    "import json, datetime\n",
    "outputs_dir = PROJECT_ROOT / \"outputs\"\n",
    "outputs_dir.mkdir(exist_ok=True)\n",
    "timestamp = datetime.datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "output_file = outputs_dir / f\"result_{timestamp}.json\"\n",
    "\n",
    "with open(output_file, \"w\", encoding=\"utf-8\") as f:\n",
    "    json.dump(result, f, ensure_ascii=False, indent=4)\n",
    "\n",
    "logging.info(f\"Saved output to {output_file}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5ed5a1c-5527-45ed-9687-9beb9b686e1c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (Ancient-AI)",
   "language": "python",
   "name": "ancient-ai-env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
